{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"glzThRiXKTZs"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","running_number = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2j8FMYEvA9A"},"outputs":[],"source":["from ctypes import Structure\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import math\n","import cmath\n","from matplotlib import pyplot as plt\n","from matplotlib.backends.backend_pdf import PdfPages\n","\n","plt.rcParams[\"figure.figsize\"] = [5, 2.5]\n","\n","second = 1\n","fs = second * 1e-15\n","ps = second * 1e-12\n","meter = 1\n","nm = meter * 1e-9\n","um = meter * 1e-6\n","mm = meter * 1e-3\n","\n","\n","pad = 500  # 500\n","N_samples = 2 ** 10  # 10000\n","c_const = 3e8 * meter / second\n","n_ref = 1\n","\n","dt = (2 * pad * fs) / N_samples\n","t_vec = np.arange(-pad * fs, pad * fs + dt, dt)\n","f_vec = np.fft.fftshift(np.fft.fftfreq(t_vec.shape[0], d=dt))\n","omega_vec = (2 * np.pi * f_vec)\n","f_s = f_vec[40]-f_vec[39]\n","\n","with open(\"/content/drive/MyDrive/Thin_layers_data/data_5layers_20000.npy\", 'rb') as f:\n","    vec_structure = np.load(f)\n","    vec_T = np.load(f)\n","    vec_R = np.load(f)\n","\n","print(vec_structure.shape)\n","print(vec_T.shape)\n","print(vec_R.shape)\n","\n","def non_zero(x_vec, y_vec, min_num=0.001):\n","  i = 0\n","  j = len(x_vec) - 1\n","  while y_vec[i] < min_num:\n","    i = i + 1\n","  while y_vec[j] < min_num:\n","    j = j - 1\n","    # k = min(i , len(x_vec) - j)\n","  return x_vec[i:j], y_vec[i:j]\n","\n","def generate_pulse(index):\n","    plt.figure()\n","    x_in_time, y_in_time = non_zero(t_vec, vec_T[index])\n","    plt.plot(x_in_time, y_in_time)\n","    x_in_time, y_in_time = non_zero(t_vec, vec_R[index])\n","    plt.xlabel(\"t [s]\")\n","    plt.ylabel(\"Intensity [a.u]\")\n","    plt.figure()\n","    plt.plot(x_in_time, y_in_time)\n","    plt.xlabel(\"t [s]\")\n","    plt.ylabel(\"Intensity [a.u]\")\n","    #plt.legend(['T', 'R'], loc='upper left')\n","    plt.show()\n","\n","def encode_refractive(vec_structure):\n","  for structure in vec_structure:\n","    if structure[0,2] == 'silicon':\n","      structure[0,0] = 0\n","    else:\n","      structure[0,0] = 1\n","  return np.array(vec_structure)\n","\n","\n","generate_pulse(index= random.randint(0,9))\n","vec_structure = vec_structure[:,1:-1,:] #without air\n","vec_structure = encode_refractive(vec_structure)\n","print(vec_structure)\n","vec_structure[:,:,1] = np.float32(vec_structure[:,:,1]) * (10**7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOXYo96YvVRR"},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","import os\n","import time\n","from keras.models import Sequential\n","from keras.layers import Dense, Input\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from keras.models import Model\n","from keras.utils.vis_utils import plot_model\n","from keras.utils.generic_utils import get_custom_objects\n","from keras.layers import Activation\n","\n","tf.keras.backend.clear_session()\n","\n","batch_size = 128\n","epochs = 60\n","random_state = 9990\n","test_size = 0.1\n","\n","x_samples = np.dstack((vec_T, vec_R))\n","y_samples = vec_structure[:,:,:2].astype(np.float32)\n","\n","x_train, x_test, y_train, y_test = train_test_split(x_samples, y_samples,\n","                                                    test_size=test_size,\n","                                                    random_state = random_state)\n","\n","x_test, x_dev, y_test, y_dev = train_test_split(x_test, y_test,\n","                                                    test_size=0.5,\n","                                                random_state=0)\n","\n","print(x_train.shape)\n","print(y_train.dtype)\n","\n","# Model\n","T_vec_input = Input(shape=x_train.shape[1])\n","R_vec_input = Input(shape=x_train.shape[1])\n","R_vec_first_layer = Dense(512, activation='relu')(R_vec_input)\n","T_vec_first_layer = Dense(512, activation='relu')(T_vec_input)\n","concat_first = tf.keras.layers.Concatenate()([R_vec_first_layer,\n","                                              T_vec_first_layer])\n","x = Dense(512, activation='relu')(concat_first)\n","x = Dense(256, activation='relu')(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dense(64, activation='relu')(x)\n","Width_output = Dense(y_train.shape[1], activation='relu')(x)\n","Refidx_output = Dense(2, activation='softmax')(x)\n","\n","model = Model(inputs=[T_vec_input, R_vec_input],\n","              outputs=[Width_output, Refidx_output])\n","\n","model.summary()\n","plot_model(model,\n","           to_file='/content/drive/MyDrive/Thin_layers_data/model_plot.png',\n","           show_shapes=True, show_layer_names=False)\n","\n","\n","# model compilation\n","model.compile(loss=['mae', 'sparse_categorical_crossentropy'],\n","              loss_weights=[1.0, 1.0],\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=['mae'])\n","\n","# save model\n","model_file = '/content/drive/MyDrive/Thin_layers_data/model.h5'\n","checkpoint = ModelCheckpoint(model_file, verbose=1, monitor='val_loss',\n","                             save_best_only=True, mode='auto')\n","\n","# model fit\n","history = model.fit([x_train[:,:,0], x_train[:,:,1]], [y_train[:,:,1], y_train[:,0,0]],\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=([x_dev[:,:,0],\n","                            x_dev[:,:,1]], [y_dev[:,:,1], y_dev[:,0,0]]),\n","          callbacks=[checkpoint])\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model mae')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","# plt.savefig('/content/drive/MyDrive/Thin_layers_data/accuracy_basic.png')\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdi997lXpTp_"},"outputs":[],"source":["loss_history = np.array(history.history['loss'])\n","val_loss_history = np.array(history.history['val_loss'])\n","\n","outfile = \"/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_\" + str(running_number) + \"_history\"\n","np.savez(outfile, loss_history = loss_history, val_loss_history = val_loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0mVMQS7pThb"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_history.npz'\n","npzfile = np.load(outfile)\n","d = dict(npzfile)\n","plt.figure()\n","plt.plot(d['loss_history'])\n","plt.plot(d['val_loss_history'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeT2ldSHvVcz"},"outputs":[],"source":["# Add graphics that compare structures and waveforms\n","\n","# START- the relevat parts form the pycharm project\n","\n","c_const = 3e8 * meter / second\n","n_ref = 1\n","\n","f_vec = np.fft.fftshift(np.fft.fftfreq(t_vec.shape[0], d=dt))\n","omega_vec = (2 * np.pi * f_vec)\n","lambda_vec = (2 * np.pi * c_const / omega_vec)\n","\n","def P_matrix(n, d, wave_length):\n","    phase = 2 * (math.pi / wave_length) * d * n\n","    P11 = cmath.exp(-1j * phase)\n","    P22 = cmath.exp(1j * phase)\n","    P = np.array([[P11, 0], [0, P22]])\n","    return P\n","\n","def T_matrix(n1, n2):\n","    n1 = n1.real\n","    n2 = n2.real\n","    T11 = (n1 + n2) / (2 * n1)\n","    T12 = (n1 - n2) / (2 * n1)\n","    T21 = (n1 - n2) / (2 * n1)\n","    T22 = (n1 + n2) / (2 * n1)\n","    T = np.array([[T11, T12], [T21, T22]])\n","    return T\n","\n","def P_list(NDlist, wave_length):\n","    lis = []\n","    for i in range(len(NDlist) - 2):\n","        n = NDlist[i + 1][0]\n","        d = NDlist[i + 1][1]\n","        lis.append(P_matrix(n, d, wave_length))\n","    return lis\n","\n","def T_list(NDlist):\n","    lis = []\n","    for i in range(len(NDlist) - 1):\n","        lis.append(T_matrix(NDlist[i][0], NDlist[i + 1][0]))\n","    return lis\n","\n","def M_matrix(NDlist, wave_length):\n","    Tlist = T_list(NDlist)\n","    Plist = P_list(NDlist, wave_length)\n","    M = np.array([[1, 0], [0, 1]])\n","    for i in range(len(Plist)):\n","        m = np.dot(Tlist[i], Plist[i])\n","        M = np.dot(M, m)\n","    M = np.dot(M, Tlist[-1])\n","    return M\n","\n","def reflectance(NDlist, wave_length):\n","    M = M_matrix(NDlist, wave_length)\n","    r = M[1][0] / M[0][0]\n","    return r\n","\n","def transmittance(NDlist, wave_length):\n","    M = M_matrix(NDlist, wave_length)\n","    t = 1 / M[0][0]\n","    return t\n","\n","def t_spectrum_vec(NDlist, vec):\n","    lis = []\n","    for i in range(vec.shape[0]):\n","        t = transmittance(NDlist, vec[i])\n","        lis.append(t)\n","    return lis\n","\n","\n","def r_spectrum_vec(NDlist, vec):\n","    lis = []\n","    for i in range(vec.shape[0]):\n","        r = reflectance(NDlist, vec[i])\n","        lis.append(r)\n","    return lis\n","\n","def generate_vector(NDlist):\n","    width_time = 12 * fs\n","    carrier_wavelength = 800 * nm\n","    omega_0 = np.divide((2 * np.pi * c_const), (n_ref * carrier_wavelength))\n","    input_wave_amp = np.exp(- 2 * np.log(2) * np.square(t_vec / (width_time)))\n","    input_wave_phase = 0\n","    input_wave_phase = input_wave_phase + omega_0 * t_vec\n","    input_wave = input_wave_amp * np.exp(1j * input_wave_phase)\n","    input_wave_spectrum = np.fft.fftshift(np.fft.fft(np.fft.ifftshift(input_wave)))\n","    reflected_lambda = r_spectrum_vec(NDlist, lambda_vec) * input_wave_spectrum\n","    transmitted_lambda = t_spectrum_vec(NDlist, lambda_vec) * input_wave_spectrum\n","    output_wave_reflected = np.fft.ifftshift(np.fft.ifft(np.fft.fftshift(reflected_lambda)))\n","    output_wave_transmitted = np.fft.ifftshift(np.fft.ifft(np.fft.fftshift(transmitted_lambda)))\n","    return (np.abs(output_wave_transmitted) ** 2), (np.abs(output_wave_reflected) ** 2)\n","\n","# END- the relevat parts form the pycharm project\n","\n","def create_NDlist(n , d):\n","  NDlist = np.zeros((len(d),2))\n","  for i in range(len(d)):\n","    if i % 2 == n:\n","      NDlist[i,0] = 3.4777237564709433\n","    else:\n","      NDlist[i,0] = 1.4440236216729967\n","  NDlist[:,1] = d\n","  return NDlist\n","\n","def plot_structure(NDlist):\n","  # d1 = int(NDlist[0][1] * (10**7))\n","  # d2 = int(NDlist[1][1] * (10**7))\n","  d = NDlist[:,1] * (10**7)\n","\n","  d_sum = int(np.sum(d))\n","  image = np.zeros((int(d_sum/3) , d_sum))\n","  n = int(NDlist[0][0] < NDlist[1][0])\n","  point = 0\n","  for i in range(len(d)-1):\n","    image[:,point:point+ int(d[i])] = 255 * ((i-n)%2)\n","    point = point + int(d[i])\n","  image[: ,point:] = 255 * ((i-n+1)%2)\n","  return image\n","\n","def add_air(NDlist):\n","  new = np.zeros((NDlist.shape[0] + 2,2))\n","  new[0,0] = 1\n","  new[NDlist.shape[0] + 1,0] = 1\n","  new[0,1] = 0\n","  new[NDlist.shape[0] + 1,1] = 0\n","  new[1:-1, :] = NDlist\n","  return new\n","\n","def calculate_correlation_structure(NDlist_original, NDlist_prediction):\n","  if (NDlist_original[0,0] > 3 and NDlist_prediction[0,0] < 3) or (NDlist_original[0,0] < 3 and NDlist_prediction[0,0] > 3):\n","    return 0\n","  num_of_layers = NDlist_original.shape[0]\n","  corr = 0\n","  for i in range(num_of_layers):\n","      corr += min(d_original[i], d_prediction[i])/max(d_original[i], d_prediction[i])\n","  corr /= num_of_layers\n","  corr *= 100\n","  corr = int(corr)/100\n","  return corr\n","\n","def correl(x,y):\n","  up = abs(x-y)\n","  down = x+y\n","  correl = 1 - np.average(up/down)\n","  return correl\n","\n","\n","########################################\n","# THE MAIN PART OF THE CODE STARTS HERE!\n","########################################\n","\n","# n_prediction = 0 or 1\n","# d_prediction = [d1 , d2, d3, ... , d20]\n","\n","mm = 10**(-6)\n","\n","for i in range(3):\n","  index = random.randint(0,100)\n","  n_original = y_test[index][0,0]\n","  d_original = y_test[index,:,1] * (10**-7)\n","\n","  y_pred = model.predict([x_test[:,:,0], x_test[:,:,1]])\n","  n_prediction_vector = np.float32(y_pred[1] < 0.5)[:,0]\n","  d_prediction_vector = y_pred[0]* (10**-7)\n","\n","  n_prediction = n_prediction_vector[index]\n","  d_prediction = d_prediction_vector[index]\n","  print(\"\\ni = {}, index = {}\".format(i, index))\n","  print(\"n original:\")\n","  print(n_original)\n","  print(\"d original:\")\n","  print(d_original)\n","  print(\"n prediction:\")\n","  print(n_prediction)\n","  print(\"d prediction:\")\n","  print(d_prediction)\n","\n","  NDlist_original = create_NDlist(n_original, d_original)\n","  NDlist_prediction = create_NDlist(n_prediction, d_prediction)\n","\n","  plt.figure(i)\n","  image_original = plot_structure(NDlist_original)\n","  image_prediction = plot_structure(NDlist_prediction)\n","  plt.subplot(2,2,1)\n","  plt.imshow(image_original)\n","  plt.title('original- one pixel = 100nm')\n","  plt.subplot(2,2,3)\n","  plt.imshow(image_prediction)\n","  plt.title('prediction- one pixel = 100nm')\n","  plt.subplot(2,2,2)\n","  x_t_original, y_t_original = non_zero(t_vec, x_test[index, :,0])\n","  x_r_original, y_r_original = non_zero(t_vec, x_test[index, :,1])\n","  plt.plot(x_t_original, y_t_original)\n","  plt.plot(x_r_original, y_r_original)\n","  plt.legend(['T' , 'R'])\n","  plt.subplot(2,2,4)\n","  y_t_prediction0, y_r_prediction0 = generate_vector(add_air(NDlist_prediction))\n","  x_t_prediction, y_t_prediction = non_zero(t_vec, y_t_prediction0)\n","  x_r_prediction, y_r_prediction = non_zero(t_vec, y_r_prediction0)\n","  plt.plot(x_t_prediction, y_t_prediction)\n","  plt.plot(x_r_prediction, y_r_prediction)\n","  plt.legend(['T' , 'R'])\n","  plt.figure()\n","  plt.subplot(2,1,1)\n","  plt.plot(x_t_original, y_t_original)\n","  plt.plot(x_t_prediction, y_t_prediction)\n","  plt.legend(['orig' , 'pred'])\n","  plt.title('T')\n","  plt.subplot(2,1,2)\n","  plt.plot(x_r_original, y_r_original)\n","  plt.plot(x_r_prediction, y_r_prediction)\n","  plt.legend(['orig' , 'pred'])\n","  plt.title('R')\n","\n","  plt.show()\n","\n","  correlation_t = int(100 * np.abs(np.corrcoef(x_test[index, :,0], y_t_prediction0)[0,1]))\n","  correlation_r = int(100 * np.abs(np.corrcoef(x_test[index, :,1], y_r_prediction0)[0,1]))\n","  correlation_structure = int(calculate_correlation_structure(NDlist_original, NDlist_prediction) * 100)\n","  print(\"correlation t = {}% \\ncorrelation r = {}% \\ncorrelation structure = {}%\".format(correlation_t, correlation_r, correlation_structure))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocupWH2qvVff"},"outputs":[],"source":["# Histogram of Correlation for the test set\n","\n","y_pred = model.predict([x_test[:,:,0], x_test[:,:,1]])\n","n_prediction_vector = np.float32(y_pred[1] < 0.5)[:,0]\n","d_prediction_vector = y_pred[0] * (10**-7)\n","\n","def prediction(index, n_prediction_vector, d_prediction_vector):\n","  n_prediction = n_prediction_vector[index]\n","  d_prediction = d_prediction_vector[index]\n","  return n_prediction, d_prediction\n","\n","def ND_to_TR(n_prediction, d_prediction):\n","  NDlist_prediction = create_NDlist(n_prediction, d_prediction)\n","  t_prediction, r_prediction = generate_vector(add_air(NDlist_prediction))\n","  return t_prediction, r_prediction\n","\n","correlation_structure = []\n","vec_t_original = x_test[:,:,0]\n","vec_r_original = x_test[:,:,1]\n","vec_t_prediction = []\n","vec_r_prediction = []\n","refractive_correlation = []\n","for i in range(len(x_test)):\n","  n_prediction, d_prediction = prediction(i, n_prediction_vector, d_prediction_vector)\n","  t_prediction, r_prediction = ND_to_TR(n_prediction, d_prediction)\n","  vec_t_prediction.append(t_prediction)\n","  vec_r_prediction.append(r_prediction)\n","  n_original = y_test[i,0,0]\n","  d_original = y_test[i,:,1]\n","  d_original = d_original * (10**-7)\n","  if n_original == n_prediction:\n","    refractive_correlation.append(1)\n","  else:\n","    refractive_correlation.append(0)\n","  NDlist_original = create_NDlist(n_original, d_original)\n","  NDlist_prediction = create_NDlist(n_prediction, d_prediction)\n","  correlation_structure.append(calculate_correlation_structure(NDlist_original, NDlist_prediction))\n","\n","vec_t_prediction = np.array(vec_t_prediction)\n","vec_r_prediction = np.array(vec_r_prediction)\n","\n","# Calculate correlation\n","correlation_vec_t = []\n","correlation_vec_r = []\n","for i in range(len(x_test)):\n","  correlation_vec_t.append(np.abs(np.corrcoef(vec_t_original[i], vec_t_prediction[i])[0,1]))\n","  correlation_vec_r.append(np.abs(np.corrcoef(vec_r_original[i], vec_r_prediction[i])[0,1]))\n","\n","correlation_structure = np.array(correlation_structure)\n","correlation_vec_t = np.array(correlation_vec_t)\n","correlation_vec_r = np.array(correlation_vec_r)\n","\n","plt.figure()\n","plt.subplot(1,4,1)\n","plt.title('T')\n","plt.hist(correlation_vec_t)\n","plt.subplot(1,4,2)\n","plt.title('R')\n","plt.hist(correlation_vec_r)\n","plt.subplot(1,4,3)\n","plt.title('Structure')\n","plt.hist(correlation_structure)\n","plt.subplot(1,4,4)\n","plt.title('Refractive')\n","plt.hist(refractive_correlation)\n","plt.show()\n","\n","print('average T: ' + str(int(np.average(correlation_vec_t) * 10000)/100))\n","print('average R: ' + str(int(np.average(correlation_vec_r) * 10000)/100))\n","print('average Structure: ' + str(int(np.average(correlation_structure) * 10000)/100))\n","print('average Refractive: ' + str(int(np.average(refractive_correlation) * 10000)/100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUNaSzDG7n4Z"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_NDlist'\n","np.savez(outfile, n = n_prediction_vector, d = d_prediction_vector)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2qAsBuE7nx8"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_NDlist.npz'\n","npzfile = np.load(outfile)\n","d = dict(npzfile)\n","print(d.keys())\n","print(d['n'].shape)\n","print(d['d'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_J9SMHFt-cF"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_examples'\n","savez_dict = {}\n","for i in range(1000):\n","  savez_dict['unknown_layers_DNN_t_original_example' + str(i)] = vec_t_original[i]\n","  savez_dict['unknown_layers_DNN_t_prediction_example' + str(i)] = vec_t_prediction[i]\n","  savez_dict['unknown_layers_DNN_r_original_example' + str(i)] = vec_r_original[i]\n","  savez_dict['unknown_layers_DNN_r_prediction_example' + str(i)] = vec_r_prediction[i]\n","np.savez(outfile, t_vec = t_vec, **savez_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4JffZy_t-Tt"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_examples.npz'\n","npzfile = np.load(outfile)\n","d = dict(npzfile)\n","plt.figure()\n","plt.plot(d['unknown_layers_DNN_r_original_example20'])\n","plt.plot(d['unknown_layers_DNN_r_prediction_example20'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbSf-cKSpqti"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_histogram'\n","np.savez(outfile, T_histogram = correlation_vec_t,\n","         R_histogram = correlation_vec_r,\n","         Structure_histogram = correlation_structure,\n","         Refractive_histogram = refractive_correlation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFlYOE62pqn3"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_histogram.npz'\n","npzfile = np.load(outfile)\n","d = dict(npzfile)\n","\n","plt.figure()\n","plt.subplot(1,4,1)\n","plt.title('T')\n","plt.hist(d['T_histogram'])\n","plt.subplot(1,4,2)\n","plt.title('R')\n","plt.hist(d['R_histogram'])\n","plt.subplot(1,4,3)\n","plt.title('Structure')\n","plt.hist(d['Structure_histogram'])\n","plt.subplot(1,4,4)\n","plt.title('Refractive')\n","plt.hist(d['Refractive_histogram'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zn9U3lxVqNRd"},"outputs":[],"source":["def subtraction(image_original, image_prediction):\n","  d = max(image_original.shape[1], image_prediction.shape[1])\n","  h = min(image_original.shape[0], image_prediction.shape[0])\n","  sub1 = np.zeros((h,d))\n","  sub1[:,:image_original.shape[1]] = image_original[:h,:]\n","  sub2 = np.zeros((h,d))\n","  sub2[:,:image_prediction.shape[1]] = image_prediction[:h,:]\n","  sub = np.abs(sub1-sub2)\n","  sub = (sub > 1) * 255\n","  sub[:,min(image_original.shape[1], image_prediction.shape[1]):] = 255\n","  return sub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GApEaBSZqNLB"},"outputs":[],"source":["#generate filter images\n","outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_filters'\n","savez_dict = {}\n","\n","n_original_vector = y_test[:,0,0]\n","d_original_vector = y_test[:,:,1] * (10**-7)\n","for i in range(1000):\n","  NDlist_original = create_NDlist(n_original_vector[i], d_original_vector[i])\n","  NDlist_prediction = create_NDlist(n_prediction_vector[i], d_prediction_vector[i])\n","  image_original = plot_structure(NDlist_original)\n","  image_prediction = plot_structure(NDlist_prediction)\n","  sub = subtraction(image_original, image_prediction)\n","  savez_dict['original' + str(i)] = image_original\n","  savez_dict['prediction' + str(i)] = image_prediction\n","  savez_dict['subtraction' + str(i)] = sub\n","\n","np.savez(outfile, **savez_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NCfvLlNqS_I"},"outputs":[],"source":["outfile = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '_filters.npz'\n","npzfile = np.load(outfile)\n","d = dict(npzfile)\n","plt.figure()\n","plt.subplot(3,1,1)\n","plt.imshow(d['original200'])\n","plt.subplot(3,1,2)\n","plt.imshow(d['prediction200'])\n","plt.subplot(3,1,3)\n","plt.imshow(d['subtraction200'])\n","plt.show()\n","print(d['original200'].shape)\n","print(d['prediction200'].shape)\n","print(d['subtraction200'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6f9cyBuvuQW"},"outputs":[],"source":["# Saving images as PDF\n","\n","plt.rcParams[\"figure.figsize\"] = [15, 7.50]\n","plt.rcParams[\"figure.autolayout\"] = True\n","\n","for i in range(300):\n","  index = i\n","  n_original = y_test[index][0,0]\n","  d_original = y_test[index,:,1] * (10**-7)\n","\n","  y_pred = model.predict([x_test[:,:,0], x_test[:,:,1]])\n","  n_prediction_vector = np.float32(y_pred[1] < 0.5)[:,0]\n","  d_prediction_vector = y_pred[0]* (10**-7)\n","\n","  n_prediction = n_prediction_vector[index]\n","  d_prediction = d_prediction_vector[index]\n","\n","  NDlist_original = create_NDlist(n_original, d_original)\n","  NDlist_prediction = create_NDlist(n_prediction, d_prediction)\n","\n","  plt.figure()\n","  image_original = plot_structure(NDlist_original)\n","  image_prediction = plot_structure(NDlist_prediction)\n","  correlation_structure = int(calculate_correlation_structure(NDlist_original, NDlist_prediction) * 100)\n","  plt.subplot(3,2,1)\n","  plt.imshow(image_original)\n","  plt.title('original- one pixel = 100nm | structure correlation: {}%'.format(correlation_structure))\n","  plt.subplot(3,2,3)\n","  plt.imshow(image_prediction)\n","  plt.title('prediction- one pixel = 100nm | structure correlation: {}%'.format(correlation_structure))\n","  plt.subplot(3,2,2)\n","  x_t_original, y_t_original = non_zero(t_vec, x_test[index, :,0])\n","  x_r_original, y_r_original = non_zero(t_vec, x_test[index, :,1])\n","  plt.plot(x_t_original, y_t_original)\n","  plt.plot(x_r_original, y_r_original)\n","  plt.legend(['T' , 'R'])\n","  plt.title(\"original\")\n","  plt.subplot(3,2,4)\n","  y_t_prediction0, y_r_prediction0 = generate_vector(add_air(NDlist_prediction))\n","  x_t_prediction, y_t_prediction = non_zero(t_vec, y_t_prediction0)\n","  x_r_prediction, y_r_prediction = non_zero(t_vec, y_r_prediction0)\n","  plt.plot(x_t_prediction, y_t_prediction)\n","  plt.plot(x_r_prediction, y_r_prediction)\n","  plt.legend(['T' , 'R'])\n","  plt.title(\"prediction\")\n","  plt.subplot(3,2,5)\n","  plt.plot(x_t_original, y_t_original)\n","  plt.plot(x_t_prediction, y_t_prediction)\n","  plt.legend(['orig' , 'pred'])\n","  correlation_t = int(100 * np.abs(np.corrcoef(x_test[index, :,0], y_t_prediction0)[0,1]))\n","  plt.title('T - correlation: {}%'.format(correlation_t))\n","  plt.subplot(3,2,6)\n","  plt.plot(x_r_original, y_r_original)\n","  plt.plot(x_r_prediction, y_r_prediction)\n","  plt.legend(['orig' , 'pred'])\n","  correlation_r = int(100 * np.abs(np.corrcoef(x_test[index, :,1], y_r_prediction0)[0,1]))\n","  plt.title('R - correlation: {}%'.format(correlation_r))\n","\n","\n","def save_multi_image(filename):\n","   pp = PdfPages(filename)\n","   fig_nums = plt.get_fignums()\n","   figs = [plt.figure(n) for n in fig_nums]\n","   for fig in figs:\n","      fig.savefig(pp, format='pdf')\n","   pp.close()\n","\n","filename = '/content/drive/MyDrive/Thin_layers_data/new_data/layers5_DNN_' + str(running_number) + '.pdf'\n","save_multi_image(filename)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}